{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial1: 그냥 버트 써서 classifier 통과시키기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "# with open(\"data/fromgithub2.json\",\"r\") as f:\n",
    "#     df = pd.read_json(f)\n",
    "#     # posts = json.loads(f.read())\n",
    "\n",
    "# df = df[[\"title\",\"time\",\"url\"]]\n",
    "# df[\"time\"] = df[\"time\"].astype(\"datetime64[s]\",)\n",
    "# df[\"visited\"] = 0\n",
    "# # df.to_csv(\"data/fromgithub2.csv\")\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "FILE = \"data/database1.csv\"\n",
    "df = pd.read_csv(FILE)\n",
    "df = df[[\"title\",\"visited\"]]\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# # features_df = df[[\"title\",\"time\",\"kids\"]]\n",
    "# # titles = df.loc[df[\"title\"].isna()==False,\"title\"]\n",
    "# df.dropna(subset=[\"title\"])\n",
    "# titles = df[\"title\"]\n",
    "\n",
    "# # features_df.loc[:,\"kids\"] = features_df[\"kids\"].apply(lambda x: len(x))\n",
    "# # features_df[\"time\"] = features_df[\"time\"].astype('datetime64[s]')\n",
    "# titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill dummy data\n",
    "# y = [1 if i%2==0 else 0 for i in range(len(features))]\n",
    "# y = np.array(y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"title\"], df[\"visited\"], test_size=0.33, random_state=RANDOM_STATE)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel \n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "# tokenized = df[\"title\"].apply(\n",
    "#     (lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "tokenized_train = X_train.apply(\n",
    "    (lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "tokenized_test = X_test.apply(\n",
    "    (lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "tmp = max([len(row) for row in tokenized_train])\n",
    "max_len = max([len(row) for row in tokenized_test])\n",
    "max_len = max(tmp, max_len)\n",
    "\n",
    "\n",
    "padded_train = np.array([i + [0]*(max_len-len(i)) for i in tokenized_train.values])\n",
    "padded_test = np.array([i + [0]*(max_len-len(i)) for i in tokenized_test.values])\n",
    "attention_mask_train = np.where(padded_train!=0 , 1, 0)\n",
    "attention_mask_test = np.where(padded_test!=0 , 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "output_train = model(\n",
    "    torch.tensor(padded_train),\n",
    "    attention_mask=torch.tensor(attention_mask_train))\n",
    "# output_test = model(torch.tensor(padded_test), attention_mask=attention_mask_test)\n",
    "\n",
    "X_train = output_train[0][:,0,:].detach().numpy()\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test = model(\n",
    "    torch.tensor(padded_test),\n",
    "    attention_mask=torch.tensor(attention_mask_test))\n",
    "\n",
    "X_test = output_test[0][:,0,:].detach().numpy()\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"DUMMY\")\n",
    "clf = DummyClassifier(random_state=RANDOM_STATE)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "# cross_val_score(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.metrics import roc_curve, roc_auc_score, RocCurveDisplay\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"LINEAR, LOGISTIC, RIDGE\")\n",
    "# clf = LinearRegression()\n",
    "# clf = RidgeClassifier()\n",
    "clfs = [\n",
    "    LinearRegression(),\n",
    "    # LogisticRegression(max_iter=2, n_jobs=-1,random_state=RANDOM_STATE),\n",
    "    # LogisticRegression(),\n",
    "    LogisticRegression(max_iter=5, solver=\"sag\", n_jobs=-1,random_state=RANDOM_STATE),\n",
    "    RidgeClassifier(),\n",
    "    GaussianNB(),\n",
    "]\n",
    "\n",
    "\n",
    "for clf in clfs:\n",
    "    print()\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"acc: \", clf.score(X_test, y_test))\n",
    "    print(\"crossval: \", cross_val_score(clf, X_test, y_test))\n",
    "    # y_score = clf.pre(X_test)[:,1]\n",
    "    y_score = clf.predict(X_test)\n",
    "    print(\"auroc: \", roc_auc_score(y_test,y_score))\n",
    "    # fpr, tpr = roc_curve(y_test, y_score)\n",
    "    # display(RocCurveDisplay(fpr, tpr, ))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial2: fine tuing bert. with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "FILE = \"data/database1.csv\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "df = pd.read_csv(FILE)\n",
    "# df[df[\"url\"]==\"\"]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     df[\"title\"], df[\"visited\"], test_size=0.33, random_state=RANDOM_STATE)\n",
    "\n",
    "# X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "BERT_TYPE = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_TYPE)\n",
    "\n",
    "# tokenized = df[\"title\"].apply(\n",
    "#     (lambda x: tokenizer(x, return_tensors=\"np\", padding=True)))\n",
    "# max_len = max([len(row) for row in tokenized])\n",
    "\n",
    "tokenized = tokenizer(df[\"title\"].to_list(), padding=True, add_special_tokens=True)\n",
    "tokenized = dict(tokenized)\n",
    "\n",
    "labels = np.array(df[\"visited\"])  # Label is already an array of 0 and 1\n",
    "print(len(tokenized[\"input_ids\"]), len(tokenized[\"input_ids\"][0]), labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(BERT_TYPE)\n",
    "model.compile(optimizer=Adam(3e-5))\n",
    "model.fit(tokenized, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 33333... with Pytorch trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = \"data/database1.csv\"\n",
    "RANDOM_STATE = 42\n",
    "BERT_TYPE = \"distilbert-base-uncased\"\n",
    "\n",
    "df = pd.read_csv(FILE)[[\"title\", \"visited\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_TYPE)\n",
    "\n",
    "tokenized = tokenizer(df[\"title\"].to_list(), padding=True, add_special_tokens=True)\n",
    "tokenized = dict(tokenized)\n",
    "labels = np.array(df[\"visited\"])  # Label is already an array of 0 and 1\n",
    "print(len(tokenized[\"input_ids\"]), len(tokenized[\"input_ids\"][0]), labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.DataFrame(tokenized)\n",
    "full_df[\"visited\"] = labels\n",
    "full_dataset = full_df.to_dict('series')\n",
    "print(full_dataset)\n",
    "# print(tokenized[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BERT_TYPE, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_df,\n",
    "    eval_dataset=eval_df,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# more fine tuning stuff with tf\n",
    "- https://team-ak.tistory.com/2\n",
    "- https://www.kaggle.com/code/manojgadde/beginner-bert-fine-tuning-disaster-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1193, 31)\n",
      "(299, 31)\n",
      "(1193,)\n",
      "(299,)\n",
      "(1193, 31)\n",
      "(299, 31)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = \"data/database1.csv\"\n",
    "RANDOM_STATE = 42\n",
    "BERT_TYPE = \"distilbert-base-uncased\"\n",
    "\n",
    "df = pd.read_csv(FILE)[[\"title\", \"visited\"]]\n",
    "# df = df[:200]\n",
    "\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(BERT_TYPE)\n",
    "\n",
    "tokenized = tokenizer(df[\"title\"].to_list(), \n",
    "                padding=True, add_special_tokens=True)\n",
    "\n",
    "input_ids = np.asarray(tokenized[\"input_ids\"])\n",
    "attention_mask = np.asarray(tokenized[\"attention_mask\"])\n",
    "labels = np.asarray(df[\"visited\"])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, mask_train, mask_test \\\n",
    "    = train_test_split(input_ids, labels, attention_mask,\n",
    "                        test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "for data in [X_train, X_test, y_train, y_test, mask_train, mask_test]:\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 14:46:54.551707: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-06 14:46:54.626469: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-06 14:46:54.627590: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-06 14:46:55.832473: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'vocab_layer_norm', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    TFBertForSequenceClassification, \n",
    "    TFDistilBertForSequenceClassification,\n",
    ")\n",
    "\n",
    "bert_original = TFDistilBertForSequenceClassification.from_pretrained(BERT_TYPE)\n",
    "bert_finetuned = TFDistilBertForSequenceClassification.from_pretrained(BERT_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 991, 1: 202})\n"
     ]
    }
   ],
   "source": [
    "# X_train.shape, y_train.shape, mask_train.shape\n",
    "import collections\n",
    "print(collections.Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 120s 3s/step - loss: 4.6374 - accuracy: 0.6697\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-3)\n",
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "# metric_auc = tf.keras.metrics.AUC(from_logits=False, multi_label=True, num_labels=2)\n",
    "\n",
    "bert_finetuned.compile(\n",
    "    # optimizer = tf.keras.optimizers.Adam(),\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "# history = bert_finetuned.fit([X_train, mask_train], y_train, epochs=1)\n",
    "history = bert_finetuned.fit(\n",
    "    X_train, y_train, \n",
    "    # batch_size=32,\n",
    "    epochs=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 10s 756ms/step - loss: 2.7858 - accuracy: 0.8194\n",
      "2.7857766151428223 0.8193979859352112\n"
     ]
    }
   ],
   "source": [
    "# loss, acc = bert_finetuned.evaluate([X_test, mask_test], y_test)\n",
    "loss, acc = bert_finetuned.evaluate([X_test, mask_test], y_test)\n",
    "print(loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 9s 751ms/step\n",
      "Counter({0: 299})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "preds = bert_finetuned.predict([X_test,mask_test])\n",
    "y_pred = np.argmax(preds[\"logits\"],axis=1)\n",
    "print(collections.Counter(y_pred))\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
