{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial1: 그냥 버트 써서 classifier 통과시키기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "# with open(\"data/fromgithub2.json\",\"r\") as f:\n",
    "#     df = pd.read_json(f)\n",
    "#     # posts = json.loads(f.read())\n",
    "\n",
    "# df = df[[\"title\",\"time\",\"url\"]]\n",
    "# df[\"time\"] = df[\"time\"].astype(\"datetime64[s]\",)\n",
    "# df[\"visited\"] = 0\n",
    "# # df.to_csv(\"data/fromgithub2.csv\")\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "FILE = \"data/database1.csv\"\n",
    "df = pd.read_csv(FILE)\n",
    "df = df[[\"title\",\"visited\"]]\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# # features_df = df[[\"title\",\"time\",\"kids\"]]\n",
    "# # titles = df.loc[df[\"title\"].isna()==False,\"title\"]\n",
    "# df.dropna(subset=[\"title\"])\n",
    "# titles = df[\"title\"]\n",
    "\n",
    "# # features_df.loc[:,\"kids\"] = features_df[\"kids\"].apply(lambda x: len(x))\n",
    "# # features_df[\"time\"] = features_df[\"time\"].astype('datetime64[s]')\n",
    "# titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill dummy data\n",
    "# y = [1 if i%2==0 else 0 for i in range(len(features))]\n",
    "# y = np.array(y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"title\"], df[\"visited\"], test_size=0.33, random_state=RANDOM_STATE)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel \n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "# tokenized = df[\"title\"].apply(\n",
    "#     (lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "tokenized_train = X_train.apply(\n",
    "    (lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "tokenized_test = X_test.apply(\n",
    "    (lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "tmp = max([len(row) for row in tokenized_train])\n",
    "max_len = max([len(row) for row in tokenized_test])\n",
    "max_len = max(tmp, max_len)\n",
    "\n",
    "\n",
    "padded_train = np.array([i + [0]*(max_len-len(i)) for i in tokenized_train.values])\n",
    "padded_test = np.array([i + [0]*(max_len-len(i)) for i in tokenized_test.values])\n",
    "attention_mask_train = np.where(padded_train!=0 , 1, 0)\n",
    "attention_mask_test = np.where(padded_test!=0 , 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "output_train = model(\n",
    "    torch.tensor(padded_train),\n",
    "    attention_mask=torch.tensor(attention_mask_train))\n",
    "# output_test = model(torch.tensor(padded_test), attention_mask=attention_mask_test)\n",
    "\n",
    "X_train = output_train[0][:,0,:].detach().numpy()\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test = model(\n",
    "    torch.tensor(padded_test),\n",
    "    attention_mask=torch.tensor(attention_mask_test))\n",
    "\n",
    "X_test = output_test[0][:,0,:].detach().numpy()\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"DUMMY\")\n",
    "clf = DummyClassifier(random_state=RANDOM_STATE)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "# cross_val_score(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.metrics import roc_curve, roc_auc_score, RocCurveDisplay\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"LINEAR, LOGISTIC, RIDGE\")\n",
    "# clf = LinearRegression()\n",
    "# clf = RidgeClassifier()\n",
    "clfs = [\n",
    "    LinearRegression(),\n",
    "    # LogisticRegression(max_iter=2, n_jobs=-1,random_state=RANDOM_STATE),\n",
    "    # LogisticRegression(),\n",
    "    LogisticRegression(max_iter=5, solver=\"sag\", n_jobs=-1,random_state=RANDOM_STATE),\n",
    "    RidgeClassifier(),\n",
    "    GaussianNB(),\n",
    "]\n",
    "\n",
    "\n",
    "for clf in clfs:\n",
    "    print()\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"acc: \", clf.score(X_test, y_test))\n",
    "    print(\"crossval: \", cross_val_score(clf, X_test, y_test))\n",
    "    # y_score = clf.pre(X_test)[:,1]\n",
    "    y_score = clf.predict(X_test)\n",
    "    print(\"auroc: \", roc_auc_score(y_test,y_score))\n",
    "    # fpr, tpr = roc_curve(y_test, y_score)\n",
    "    # display(RocCurveDisplay(fpr, tpr, ))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial2: fine tuing bert. with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "FILE = \"data/database1.csv\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "df = pd.read_csv(FILE)\n",
    "# df[df[\"url\"]==\"\"]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     df[\"title\"], df[\"visited\"], test_size=0.33, random_state=RANDOM_STATE)\n",
    "\n",
    "# X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "BERT_TYPE = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_TYPE)\n",
    "\n",
    "# tokenized = df[\"title\"].apply(\n",
    "#     (lambda x: tokenizer(x, return_tensors=\"np\", padding=True)))\n",
    "# max_len = max([len(row) for row in tokenized])\n",
    "\n",
    "tokenized = tokenizer(df[\"title\"].to_list(), padding=True, add_special_tokens=True)\n",
    "tokenized = dict(tokenized)\n",
    "\n",
    "labels = np.array(df[\"visited\"])  # Label is already an array of 0 and 1\n",
    "print(len(tokenized[\"input_ids\"]), len(tokenized[\"input_ids\"][0]), labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(BERT_TYPE)\n",
    "model.compile(optimizer=Adam(3e-5))\n",
    "model.fit(tokenized, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 33333... with Pytorch trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = \"data/database1.csv\"\n",
    "RANDOM_STATE = 42\n",
    "BERT_TYPE = \"distilbert-base-uncased\"\n",
    "\n",
    "df = pd.read_csv(FILE)[[\"title\", \"visited\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_TYPE)\n",
    "\n",
    "tokenized = tokenizer(df[\"title\"].to_list(), padding=True, add_special_tokens=True)\n",
    "tokenized = dict(tokenized)\n",
    "labels = np.array(df[\"visited\"])  # Label is already an array of 0 and 1\n",
    "print(len(tokenized[\"input_ids\"]), len(tokenized[\"input_ids\"][0]), labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.DataFrame(tokenized)\n",
    "full_df[\"visited\"] = labels\n",
    "full_dataset = full_df.to_dict('series')\n",
    "print(full_dataset)\n",
    "# print(tokenized[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BERT_TYPE, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_df,\n",
    "    eval_dataset=eval_df,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# more fine tuning stuff with tf\n",
    "- https://team-ak.tistory.com/2\n",
    "- https://www.kaggle.com/code/manojgadde/beginner-bert-fine-tuning-disaster-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = \"data/database1.csv\"\n",
    "RANDOM_STATE = 42\n",
    "BERT_TYPE = \"distilbert-base-uncased\"\n",
    "\n",
    "df = pd.read_csv(FILE)[[\"title\", \"visited\"]]\n",
    "# df = df[:200]\n",
    "\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(BERT_TYPE)\n",
    "# tokenized = tokenizer(df[\"title\"].to_list(), return_tensors=\"np\", padding=True)\n",
    "# print(tokenized)\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(BERT_TYPE)\n",
    "tokenized = tokenizer(df[\"title\"].to_list(), \n",
    "                padding=True, add_special_tokens=True)\n",
    "\n",
    "input_ids = np.asarray(tokenized[\"input_ids\"])\n",
    "attention_mask = np.asarray(tokenized[\"attention_mask\"])\n",
    "labels = np.asarray(df[\"visited\"])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, mask_train, mask_test \\\n",
    "    = train_test_split(input_ids, labels, attention_mask,\n",
    "                        test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "for data in [X_train, X_test, y_train, y_test, mask_train, mask_test]:\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    TFBertForSequenceClassification, \n",
    "    TFDistilBertForSequenceClassification,\n",
    ")\n",
    "\n",
    "bert_original = TFDistilBertForSequenceClassification.from_pretrained(BERT_TYPE)\n",
    "bert_finetuned = TFDistilBertForSequenceClassification.from_pretrained(BERT_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape, y_train.shape, mask_train.shape\n",
    "import collections\n",
    "print(collections.Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=2e-3)\n",
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "# optimizer = tf.keras.optimizers.Adagrad()\n",
    "# metric_auc = tf.keras.metrics.AUC(from_logits=False, multi_label=True, num_labels=2)\n",
    "\n",
    "bert_finetuned.compile(\n",
    "    # optimizer=optimizer,\n",
    "    optimizer = tf.keras.optimizers.Adam(3e-5),\n",
    "    # loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "# history = bert_finetuned.fit([X_train, mask_train], y_train, epochs=1)\n",
    "history = bert_finetuned.fit(\n",
    "    X_train, y_train, \n",
    "    # batch_size=32,\n",
    "    epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, acc = bert_finetuned.evaluate([X_test, mask_test], y_test)\n",
    "# print(loss, acc)\n",
    "\n",
    "xxx = bert_finetuned.evaluate(X_test, y_test)\n",
    "print(xxx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "preds = bert_finetuned.predict([X_test,mask_test])\n",
    "y_pred = np.argmax(preds[\"logits\"],axis=1)\n",
    "print(collections.Counter(y_pred))\n",
    "roc_auc_score(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model with keras\n",
    "- https://huggingface.co/docs/transformers/training#train-a-tensorflow-model-with-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999, 2) (493, 2)\n",
      "visited\n",
      "0    828\n",
      "1    171\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "visited\n",
       "0    828\n",
       "1    171\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = \"data/database1.csv\"\n",
    "RANDOM_STATE = 42\n",
    "BERT_TYPE = \"distilbert-base-uncased\"\n",
    "\n",
    "df = pd.read_csv(FILE)[[\"title\", \"visited\"]]\n",
    "# df = df[:200]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.33, random_state=RANDOM_STATE)\n",
    "print(train.shape, test.shape)\n",
    "print(train[\"visited\"].value_counts())\n",
    "\n",
    "# ############# imbalanced data. smote. ##########\n",
    "# print(\"IMBALANCED: smote\")\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# smote = SMOTE(random_state=RANDOM_STATE)\n",
    "# X, y = smote.fit_resample(X, y)\n",
    "# print(f\"{y.count(1)} {y.count(0)}\")\n",
    "# # ############# imbalanced data. dumb method. ##########\n",
    "# print(\"IMBALANCED: just make copies\")\n",
    "# tmp = train[train[\"visited\"]==1]\n",
    "# train = pd.concat([train,tmp])\n",
    "# train = pd.concat([train,tmp])\n",
    "# train = pd.concat([train,tmp])\n",
    "# train = pd.concat([train,tmp])\n",
    "# display(train[\"visited\"].value_counts())\n",
    "\n",
    "X_train, y_train = train[\"title\"], train[\"visited\"]\n",
    "X_test, y_test = test[\"title\"], test[\"visited\"]\n",
    "\n",
    "display(train[\"visited\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, AutoTokenizer\n",
    "\n",
    "def tokenize(X):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BERT_TYPE)\n",
    "    tokenized = tokenizer(X.to_list(), return_tensors=\"np\", padding=True)\n",
    "    # tokenized = dict(tokenized)\n",
    "    return dict(tokenized)\n",
    "\n",
    "def preprocess_all(a,b,c,d):\n",
    "    \"\"\" returns x_train, x_test, y_train, y_test \"\"\"\n",
    "    return tokenize(a), tokenize(b), np.array(c), np.array(d)\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_all(\n",
    "    X_train, X_test, y_train, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 19:11:19.086665: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-06 19:11:19.158934: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-06 19:11:19.159850: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-06 19:11:20.223362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'activation_13', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "32/32 [==============================] - 104s 3s/step - loss: 0.4949 - acc: 0.8258\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 85s 3s/step - loss: 0.4279 - acc: 0.8288\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 87s 3s/step - loss: 0.3559 - acc: 0.8418\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 85s 3s/step - loss: 0.1912 - acc: 0.9369\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 85s 3s/step - loss: 0.0968 - acc: 0.9660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa4a04cf550>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and compile our model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(BERT_TYPE)\n",
    "\n",
    "# Lower learning rates are often better for fine-tuning transformers\n",
    "model.compile(\n",
    "    optimizer=Adam(3e-5),\n",
    "    metrics=[\"acc\"]\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 13s 682ms/step\n",
      "16/16 [==============================] - 13s 683ms/step - loss: 0.7500 - acc: 0.7748\n",
      "[0.7499707937240601, 0.7748478651046753]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred[\"logits\"],axis=1)\n",
    "\n",
    "tup = model.evaluate(X_test, y_test)\n",
    "print(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5333333333333334\n",
      "0.20143884892086328\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "# print(collections.Counter(y_pred))\n",
    "print(roc_auc_score(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
