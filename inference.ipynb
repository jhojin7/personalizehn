{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "- 제일 밑에 있는 `# train model with keras`가 최종 버젼 입니다.\n",
    "- 최신 코드는 GitHub에 있습니다. (https://github.com/jhojin7/personalizehn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial1: 그냥 버트 써서 classifier 통과시키기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "# with open(\"data/fromgithub2.json\",\"r\") as f:\n",
    "#     df = pd.read_json(f)\n",
    "#     # posts = json.loads(f.read())\n",
    "\n",
    "# df = df[[\"title\",\"time\",\"url\"]]\n",
    "# df[\"time\"] = df[\"time\"].astype(\"datetime64[s]\",)\n",
    "# df[\"visited\"] = 0\n",
    "# # df.to_csv(\"data/fromgithub2.csv\")\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "FILE = \"data/database1.csv\"\n",
    "df = pd.read_csv(FILE)\n",
    "df = df[[\"title\",\"visited\"]]\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# # features_df = df[[\"title\",\"time\",\"kids\"]]\n",
    "# # titles = df.loc[df[\"title\"].isna()==False,\"title\"]\n",
    "# df.dropna(subset=[\"title\"])\n",
    "# titles = df[\"title\"]\n",
    "\n",
    "# # features_df.loc[:,\"kids\"] = features_df[\"kids\"].apply(lambda x: len(x))\n",
    "# # features_df[\"time\"] = features_df[\"time\"].astype('datetime64[s]')\n",
    "# titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill dummy data\n",
    "# y = [1 if i%2==0 else 0 for i in range(len(features))]\n",
    "# y = np.array(y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"title\"], df[\"visited\"], test_size=0.33, random_state=RANDOM_STATE)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel \n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "# tokenized = df[\"title\"].apply(\n",
    "#     (lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "tokenized_train = X_train.apply(\n",
    "    (lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "tokenized_test = X_test.apply(\n",
    "    (lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "tmp = max([len(row) for row in tokenized_train])\n",
    "max_len = max([len(row) for row in tokenized_test])\n",
    "max_len = max(tmp, max_len)\n",
    "\n",
    "\n",
    "padded_train = np.array([i + [0]*(max_len-len(i)) for i in tokenized_train.values])\n",
    "padded_test = np.array([i + [0]*(max_len-len(i)) for i in tokenized_test.values])\n",
    "attention_mask_train = np.where(padded_train!=0 , 1, 0)\n",
    "attention_mask_test = np.where(padded_test!=0 , 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "output_train = model(\n",
    "    torch.tensor(padded_train),\n",
    "    attention_mask=torch.tensor(attention_mask_train))\n",
    "# output_test = model(torch.tensor(padded_test), attention_mask=attention_mask_test)\n",
    "\n",
    "X_train = output_train[0][:,0,:].detach().numpy()\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test = model(\n",
    "    torch.tensor(padded_test),\n",
    "    attention_mask=torch.tensor(attention_mask_test))\n",
    "\n",
    "X_test = output_test[0][:,0,:].detach().numpy()\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"DUMMY\")\n",
    "clf = DummyClassifier(random_state=RANDOM_STATE)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "# cross_val_score(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.metrics import roc_curve, roc_auc_score, RocCurveDisplay\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"LINEAR, LOGISTIC, RIDGE\")\n",
    "# clf = LinearRegression()\n",
    "# clf = RidgeClassifier()\n",
    "clfs = [\n",
    "    LinearRegression(),\n",
    "    # LogisticRegression(max_iter=2, n_jobs=-1,random_state=RANDOM_STATE),\n",
    "    # LogisticRegression(),\n",
    "    LogisticRegression(max_iter=5, solver=\"sag\", n_jobs=-1,random_state=RANDOM_STATE),\n",
    "    RidgeClassifier(),\n",
    "    GaussianNB(),\n",
    "]\n",
    "\n",
    "\n",
    "for clf in clfs:\n",
    "    print()\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"acc: \", clf.score(X_test, y_test))\n",
    "    print(\"crossval: \", cross_val_score(clf, X_test, y_test))\n",
    "    # y_score = clf.pre(X_test)[:,1]\n",
    "    y_score = clf.predict(X_test)\n",
    "    print(\"auroc: \", roc_auc_score(y_test,y_score))\n",
    "    # fpr, tpr = roc_curve(y_test, y_score)\n",
    "    # display(RocCurveDisplay(fpr, tpr, ))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial2: fine tuing bert. with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "FILE = \"data/database1.csv\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "df = pd.read_csv(FILE)\n",
    "# df[df[\"url\"]==\"\"]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     df[\"title\"], df[\"visited\"], test_size=0.33, random_state=RANDOM_STATE)\n",
    "\n",
    "# X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "BERT_TYPE = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_TYPE)\n",
    "\n",
    "# tokenized = df[\"title\"].apply(\n",
    "#     (lambda x: tokenizer(x, return_tensors=\"np\", padding=True)))\n",
    "# max_len = max([len(row) for row in tokenized])\n",
    "\n",
    "tokenized = tokenizer(df[\"title\"].to_list(), padding=True, add_special_tokens=True)\n",
    "tokenized = dict(tokenized)\n",
    "\n",
    "labels = np.array(df[\"visited\"])  # Label is already an array of 0 and 1\n",
    "print(len(tokenized[\"input_ids\"]), len(tokenized[\"input_ids\"][0]), labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(BERT_TYPE)\n",
    "model.compile(optimizer=Adam(3e-5))\n",
    "model.fit(tokenized, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 33333... with Pytorch trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = \"data/database1.csv\"\n",
    "RANDOM_STATE = 42\n",
    "BERT_TYPE = \"distilbert-base-uncased\"\n",
    "\n",
    "df = pd.read_csv(FILE)[[\"title\", \"visited\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_TYPE)\n",
    "\n",
    "tokenized = tokenizer(df[\"title\"].to_list(), padding=True, add_special_tokens=True)\n",
    "tokenized = dict(tokenized)\n",
    "labels = np.array(df[\"visited\"])  # Label is already an array of 0 and 1\n",
    "print(len(tokenized[\"input_ids\"]), len(tokenized[\"input_ids\"][0]), labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.DataFrame(tokenized)\n",
    "full_df[\"visited\"] = labels\n",
    "full_dataset = full_df.to_dict('series')\n",
    "print(full_dataset)\n",
    "# print(tokenized[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(BERT_TYPE, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_df,\n",
    "    eval_dataset=eval_df,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# more fine tuning stuff with tf\n",
    "- https://team-ak.tistory.com/2\n",
    "- https://www.kaggle.com/code/manojgadde/beginner-bert-fine-tuning-disaster-tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = \"data/database1.csv\"\n",
    "RANDOM_STATE = 42\n",
    "BERT_TYPE = \"distilbert-base-uncased\"\n",
    "\n",
    "df = pd.read_csv(FILE)[[\"title\", \"visited\"]]\n",
    "# df = df[:200]\n",
    "\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(BERT_TYPE)\n",
    "# tokenized = tokenizer(df[\"title\"].to_list(), return_tensors=\"np\", padding=True)\n",
    "# print(tokenized)\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(BERT_TYPE)\n",
    "tokenized = tokenizer(df[\"title\"].to_list(), \n",
    "                padding=True, add_special_tokens=True)\n",
    "\n",
    "input_ids = np.asarray(tokenized[\"input_ids\"])\n",
    "attention_mask = np.asarray(tokenized[\"attention_mask\"])\n",
    "labels = np.asarray(df[\"visited\"])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test, mask_train, mask_test \\\n",
    "    = train_test_split(input_ids, labels, attention_mask,\n",
    "                        test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "for data in [X_train, X_test, y_train, y_test, mask_train, mask_test]:\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    TFBertForSequenceClassification, \n",
    "    TFDistilBertForSequenceClassification,\n",
    ")\n",
    "\n",
    "bert_original = TFDistilBertForSequenceClassification.from_pretrained(BERT_TYPE)\n",
    "bert_finetuned = TFDistilBertForSequenceClassification.from_pretrained(BERT_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape, y_train.shape, mask_train.shape\n",
    "import collections\n",
    "print(collections.Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=2e-3)\n",
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "# optimizer = tf.keras.optimizers.Adagrad()\n",
    "# metric_auc = tf.keras.metrics.AUC(from_logits=False, multi_label=True, num_labels=2)\n",
    "\n",
    "bert_finetuned.compile(\n",
    "    # optimizer=optimizer,\n",
    "    optimizer = tf.keras.optimizers.Adam(3e-5),\n",
    "    # loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "# history = bert_finetuned.fit([X_train, mask_train], y_train, epochs=1)\n",
    "history = bert_finetuned.fit(\n",
    "    X_train, y_train, \n",
    "    # batch_size=32,\n",
    "    epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, acc = bert_finetuned.evaluate([X_test, mask_test], y_test)\n",
    "# print(loss, acc)\n",
    "\n",
    "xxx = bert_finetuned.evaluate(X_test, y_test)\n",
    "print(xxx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "preds = bert_finetuned.predict([X_test,mask_test])\n",
    "y_pred = np.argmax(preds[\"logits\"],axis=1)\n",
    "print(collections.Counter(y_pred))\n",
    "roc_auc_score(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model with keras\n",
    "- +data duplication\n",
    "- https://huggingface.co/docs/transformers/training#train-a-tensorflow-model-with-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visited\n",
      "0    1404\n",
      "1     388\n",
      "Name: count, dtype: int64\n",
      "visited\n",
      "0    931\n",
      "1    269\n",
      "Name: count, dtype: int64\n",
      "(1200, 2) (592, 2)\n",
      "IMBALANCED: just make copies\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "visited\n",
       "0    931\n",
       "1    807\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "FILE = \"data/database1.csv\"\n",
    "RANDOM_STATE = 42\n",
    "BERT_TYPE = \"distilbert-base-uncased\"\n",
    "\n",
    "df = pd.read_csv(FILE)[[\"title\", \"visited\"]]\n",
    "# df = df[:200]\n",
    "###############################\n",
    "addon_df = pd.read_csv(\"data/databaseFF.csv\")[[\"title\",\"visited\"]]\n",
    "df = pd.concat([df,addon_df])\n",
    "###############################\n",
    "\n",
    "print(df[\"visited\"].value_counts())\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.33, random_state=RANDOM_STATE)\n",
    "print(train[\"visited\"].value_counts())\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "# ############# imbalanced data. smote. ##########\n",
    "# print(\"IMBALANCED: smote\")\n",
    "# from imblearn.over_sampling import SMOTE # smote not ok for BERT...\n",
    "# smote = SMOTE(random_state=RANDOM_STATE)\n",
    "# X, y = smote.fit_resample(X, y)\n",
    "# print(f\"{y.count(1)} {y.count(0)}\")\n",
    "# ############# imbalanced data. dumb method. ##########\n",
    "print(\"IMBALANCED: just make copies\")\n",
    "tmp = train[train[\"visited\"]==1]\n",
    "train = pd.concat([train,tmp])\n",
    "train = pd.concat([train,tmp])\n",
    "# train = pd.concat([train,tmp])\n",
    "# train = pd.concat([train,tmp])\n",
    "display(train[\"visited\"].value_counts())\n",
    "\n",
    "X_train, y_train = train[\"title\"], train[\"visited\"]\n",
    "X_test, y_test = test[\"title\"], test[\"visited\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>visited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>Ask HN: How to safely execute user code in bro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>Show HN: Only API Web (Or with JSON). Experime...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>ByteDance asks federal apeals court to vacate ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>How do I successfully outsource my MVP?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>Trick to Use iPhone Flashlight as Morse Code G...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Firefox displayed a pop-up ad for Mozilla VPN ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>Tell HN: I Installed Windows 10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Apartment rents fall as new supply hits market...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>A Linux sysadmin's introduction to cgroups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Raspberry Pi security alarm – the basics (cave...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1738 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  visited\n",
       "428   Ask HN: How to safely execute user code in bro...        0\n",
       "1353  Show HN: Only API Web (Or with JSON). Experime...        1\n",
       "613   ByteDance asks federal apeals court to vacate ...        0\n",
       "31              How do I successfully outsource my MVP?        1\n",
       "1270  Trick to Use iPhone Flashlight as Morse Code G...        0\n",
       "...                                                 ...      ...\n",
       "92    Firefox displayed a pop-up ad for Mozilla VPN ...        1\n",
       "805                     Tell HN: I Installed Windows 10        1\n",
       "23    Apartment rents fall as new supply hits market...        1\n",
       "1332         A Linux sysadmin's introduction to cgroups        1\n",
       "193   Raspberry Pi security alarm – the basics (cave...        1\n",
       "\n",
       "[1738 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, AutoTokenizer\n",
    "\n",
    "def tokenize(X):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BERT_TYPE)\n",
    "    tokenized = tokenizer(X.to_list(), return_tensors=\"np\", padding=True)\n",
    "    # tokenized = dict(tokenized)\n",
    "    return dict(tokenized)\n",
    "\n",
    "def preprocess_all(a,b,c,d):\n",
    "    \"\"\" returns x_train, x_test, y_train, y_test \"\"\"\n",
    "    return tokenize(a), tokenize(b), np.array(c), np.array(d)\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_all(\n",
    "    X_train, X_test, y_train, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 13:10:44.540787: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-07 13:10:44.612000: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-07 13:10:44.612998: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-07 13:10:45.663371: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'activation_13', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "14/14 [==============================] - 170s 11s/step - loss: 0.6702 - acc: 0.5955\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 150s 11s/step - loss: 0.5743 - acc: 0.7054\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 149s 11s/step - loss: 0.4490 - acc: 0.8026\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 149s 11s/step - loss: 0.2798 - acc: 0.8895\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 149s 11s/step - loss: 0.1493 - acc: 0.9419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc07c04e620>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and compile our model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(BERT_TYPE)\n",
    "\n",
    "# Lower learning rates are often better for fine-tuning transformers\n",
    "model.compile(\n",
    "    optimizer=Adam(3e-5),\n",
    "    metrics=[\"acc\"],\n",
    "    # loss=\"binary_crossentropy\",\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 19s 888ms/step\n",
      "19/19 [==============================] - 19s 889ms/step - loss: 0.7619 - acc: 0.7635\n",
      "loss, acc: [0.7618998289108276, 0.7635135054588318]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred[\"logits\"],axis=1)\n",
    "\n",
    "tup = model.evaluate(X_test, y_test)\n",
    "print(\"loss, acc:\", tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.34782608695652173\n",
      "recall: 0.20168067226890757\n",
      "roc_auc_score: 0.5532716257750457\n",
      "f1_score: 0.25531914893617025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[428,  45],\n",
       "       [ 95,  24]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import collections\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# print(collections.Counter(y_pred))\n",
    "print(\"precision:\", precision_score(y_test,y_pred))\n",
    "print(\"recall:\", recall_score(y_test,y_pred))\n",
    "print(\"roc_auc_score:\", roc_auc_score(y_test, y_pred))\n",
    "print(\"f1_score:\", f1_score(y_test, y_pred))\n",
    "display(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "0 \t AI will save the world? (pmarca.substack.com)\n",
      "1 \t OpenGL 3.1 on Asahi Linux (asahilinux.org)\n",
      "1 \t React, but in Python (github.com/reactive-python)\n",
      "0 \t Deadly heart attacks are more common on a Monday (bhf.org.uk)\n",
      "1 \t Reddit’s plan to kill third-party apps sparks widespread protests (arstechnica.com)\n",
      "0 \t MeZO: Fine-Tuning Language Models with Just Forward Passes (github.com/princeton-nlp)\n",
      "0 \t SEC asks for emergency order to freeze Binance US assets anywhere in the world (cnbc.com)\n",
      "0 \t Notes on Vision Pro (andymatuschak.org)\n",
      "1 \t GPT Best Practices (openai.com)\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "    \"AI will save the world? (pmarca.substack.com)\",\n",
    "    \"OpenGL 3.1 on Asahi Linux (asahilinux.org)\",\n",
    "    \"React, but in Python (github.com/reactive-python)\",\n",
    "    \"Deadly heart attacks are more common on a Monday (bhf.org.uk)\",\n",
    "    \"Reddit’s plan to kill third-party apps sparks widespread protests (arstechnica.com)\",\n",
    "    \"MeZO: Fine-Tuning Language Models with Just Forward Passes (github.com/princeton-nlp)\",\n",
    "    \"SEC asks for emergency order to freeze Binance US assets anywhere in the world (cnbc.com)\",\n",
    "    \"Notes on Vision Pro (andymatuschak.org)\",\n",
    "    \"GPT Best Practices (openai.com)\",\n",
    "]\n",
    "\n",
    "def tokenize(X):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BERT_TYPE)\n",
    "    tokenized = tokenizer(X, return_tensors=\"np\", padding=True)\n",
    "    # tokenized = dict(tokenized)\n",
    "    return dict(tokenized)\n",
    "\n",
    "preds = model.predict(tokenize(samples))\n",
    "preds = np.argmax(preds[\"logits\"],axis=1)\n",
    "\n",
    "for sample,pred in zip(samples,preds):\n",
    "    print(pred,\"\\t\",sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 60ms/step\n",
      "0 \t Royal Navy says quantum navigation test a success (thequantuminsider.com)\n"
     ]
    }
   ],
   "source": [
    "bad_samples = [\n",
    "    \"Royal Navy says quantum navigation test a success (thequantuminsider.com)\",\n",
    "]\n",
    "\n",
    "def tokenize(X):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BERT_TYPE)\n",
    "    tokenized = tokenizer(X, return_tensors=\"np\", padding=True)\n",
    "    # tokenized = dict(tokenized)\n",
    "    return dict(tokenized)\n",
    "\n",
    "preds = model.predict(tokenize(bad_samples))\n",
    "preds = np.argmax(preds[\"logits\"],axis=1)\n",
    "\n",
    "for sample,pred in zip(bad_samples,preds):\n",
    "    print(pred,\"\\t\",sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
