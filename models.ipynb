{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0.1', 'Unnamed: 0', 'id', 'rank', 'title', 'url', 'points',\n",
      "       'comments', 'viewed', 'liked'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>points</th>\n",
       "      <th>comments</th>\n",
       "      <th>viewed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The greatest risk of AI is from the people who...</td>\n",
       "      <td>194</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Launch HN: Onu (YC W23) – Turn scripts into in...</td>\n",
       "      <td>69</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OpenAI's plans according to sama (humanloop.com)</td>\n",
       "      <td>63</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rarbg Is No More (archive.org)</td>\n",
       "      <td>800</td>\n",
       "      <td>452</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Slide to Unlock (uwaterloo.ca)</td>\n",
       "      <td>135</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Zulip 7.0: Threaded open-source team chat (zul...</td>\n",
       "      <td>46</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Improving Mathematical Reasoning with Process ...</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Tiny, medical robots could one day travel thro...</td>\n",
       "      <td>190</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>macOS Like Fonts on Manjaro/Arch Linux (aswinm...</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rise: Accelerate the Development of Open Sourc...</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>When LIMIT 9 works but LIMIT 10 hangs: A short...</td>\n",
       "      <td>122</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>America Has Too Much Pork (wsj.com)</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>You can link an OpenPGP key to a German eID (g...</td>\n",
       "      <td>-999</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Zeal is an offline documentation browser for s...</td>\n",
       "      <td>266</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AI21 Labs concludes largest Turing Test experi...</td>\n",
       "      <td>61</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Rde: A configuration framework for GNU Guix (y...</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AI Camera with No Lens (theprompt.io)</td>\n",
       "      <td>170</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Reddit API Pricing Would Cost Apollo Developer...</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Show HN: Micro Chat – Private group chat (micr...</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>India cuts periodic table and evolution from s...</td>\n",
       "      <td>153</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ES modules: A cartoon deep-dive (2018) (hacks....</td>\n",
       "      <td>82</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Tell HN: YouTube Download Websites Disappearin...</td>\n",
       "      <td>52</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Hacking my “smart” toothbrush (kuenzi.dev)</td>\n",
       "      <td>627</td>\n",
       "      <td>247</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Some models of Gigabyte motherboards download ...</td>\n",
       "      <td>188</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Rocket Carrying North Korean Spy Satellite Cra...</td>\n",
       "      <td>57</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AI intensifies fight against ‘paper mills’ tha...</td>\n",
       "      <td>129</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AVX512 intrinsics for JDK’s Arrays.sort method...</td>\n",
       "      <td>132</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Hardy Fox, of the avant-garde band The Residen...</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Ask HN: Is it just me or GPT-4's quality has s...</td>\n",
       "      <td>784</td>\n",
       "      <td>633</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  points  comments   \n",
       "0   The greatest risk of AI is from the people who...     194        77  \\\n",
       "1   Launch HN: Onu (YC W23) – Turn scripts into in...      69        41   \n",
       "2    OpenAI's plans according to sama (humanloop.com)      63        25   \n",
       "3                      Rarbg Is No More (archive.org)     800       452   \n",
       "4                      Slide to Unlock (uwaterloo.ca)     135        28   \n",
       "5   Zulip 7.0: Threaded open-source team chat (zul...      46        11   \n",
       "6   Improving Mathematical Reasoning with Process ...      50         7   \n",
       "7   Tiny, medical robots could one day travel thro...     190        79   \n",
       "8   macOS Like Fonts on Manjaro/Arch Linux (aswinm...       9         3   \n",
       "9   Rise: Accelerate the Development of Open Sourc...      35         6   \n",
       "10  When LIMIT 9 works but LIMIT 10 hangs: A short...     122        49   \n",
       "11                America Has Too Much Pork (wsj.com)      11         4   \n",
       "12  You can link an OpenPGP key to a German eID (g...    -999         4   \n",
       "13  Zeal is an offline documentation browser for s...     266        90   \n",
       "14  AI21 Labs concludes largest Turing Test experi...      61        19   \n",
       "15  Rde: A configuration framework for GNU Guix (y...      17         2   \n",
       "16              AI Camera with No Lens (theprompt.io)     170        55   \n",
       "17  Reddit API Pricing Would Cost Apollo Developer...      26         6   \n",
       "18  Show HN: Micro Chat – Private group chat (micr...      16        14   \n",
       "19  India cuts periodic table and evolution from s...     153       104   \n",
       "20  ES modules: A cartoon deep-dive (2018) (hacks....      82        22   \n",
       "21  Tell HN: YouTube Download Websites Disappearin...      52        17   \n",
       "22         Hacking my “smart” toothbrush (kuenzi.dev)     627       247   \n",
       "23  Some models of Gigabyte motherboards download ...     188       105   \n",
       "24  Rocket Carrying North Korean Spy Satellite Cra...      57        35   \n",
       "25  AI intensifies fight against ‘paper mills’ tha...     129       115   \n",
       "26  AVX512 intrinsics for JDK’s Arrays.sort method...     132       125   \n",
       "27  Hardy Fox, of the avant-garde band The Residen...      22         4   \n",
       "28  Ask HN: Is it just me or GPT-4's quality has s...     784       633   \n",
       "\n",
       "    viewed  \n",
       "0        0  \n",
       "1        1  \n",
       "2        0  \n",
       "3        1  \n",
       "4        0  \n",
       "5        0  \n",
       "6        0  \n",
       "7        0  \n",
       "8        0  \n",
       "9        0  \n",
       "10       1  \n",
       "11       0  \n",
       "12       1  \n",
       "13       0  \n",
       "14       0  \n",
       "15       0  \n",
       "16       0  \n",
       "17       0  \n",
       "18       1  \n",
       "19       1  \n",
       "20       0  \n",
       "21       1  \n",
       "22       1  \n",
       "23       0  \n",
       "24       0  \n",
       "25       0  \n",
       "26       0  \n",
       "27       0  \n",
       "28       0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# df = pd.read_csv(\"data.csv\")\n",
    "df = pd.read_csv(\"data.csv.bak\")\n",
    "print(df.columns)\n",
    "df = df[[\"title\", \"points\", \"comments\", \"viewed\"]]\n",
    "df[\"viewed\"] = df[\"viewed\"].apply((lambda x: 1 if x==1 else 0))\n",
    "df.loc[df[\"viewed\"].isna(), \"viewed\"] = 0\n",
    "df.loc[[1,3,10,12,18,19,21],\"viewed\"] = 1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n",
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing DistilBertModel: ['bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'cls.predictions.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'cls.predictions.decoder.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.7.intermediate.dense.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.7.ffn.lin1.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.11.sa_layer_norm.bias', 'embeddings.word_embeddings.weight', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.2.output_layer_norm.weight', 'embeddings.position_embeddings.weight', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'embeddings.LayerNorm.bias', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.8.output_layer_norm.weight', 'embeddings.LayerNorm.weight', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.9.attention.q_lin.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 33)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  101,  1996,  4602,  3891,  1997,  9932,  2003,  2013,  1996,\n",
       "         2111,  2040,  2491,  2009,  1010,  2025,  1996,  6627,  2993,\n",
       "         1006,  9932,  2015,  2532,  3489, 10448,  2140,  1012,  4942,\n",
       "         9153,  3600,  1012,  4012,  1007,   102],\n",
       "       [  101,  4888,  1044,  2078,  1024,  2006,  2226,  1006,  1061,\n",
       "         2278,  1059, 21926,  1007,  1516,  2735, 14546,  2046,  4722,\n",
       "         5906,  1999,  2781,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  2330,  4886,  1005,  1055,  3488,  2429,  2000,  3520,\n",
       "         2050,  1006,  2529,  4135,  7361,  1012,  4012,  1007,   102,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101, 10958, 15185,  2290,  2003,  2053,  2062,  1006,  8756,\n",
       "         1012,  8917,  1007,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  7358,  2000, 19829,  1006,  1057,  5880,  4135,  2080,\n",
       "         1012,  6187,  1007,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101, 16950, 15000,  1021,  1012,  1014,  1024, 26583,  2330,\n",
       "         1011,  3120,  2136, 11834,  1006, 16950, 15000,  1012,  4012,\n",
       "         1007,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  9229,  8045, 13384,  2007,  2832, 10429,  1006,  2330,\n",
       "         4886,  1012,  4012,  1007,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  4714,  1010,  2966, 13507,  2071,  2028,  2154,  3604,\n",
       "         2083,  1996,  2303,  1006,  5169,  1012,  3968,  2226,  1007,\n",
       "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  6097,  2891,  2066, 15489,  2015,  2006,  2158, 16084,\n",
       "         2080,  1013,  7905, 11603,  1006,  2004, 10105,  5302,  4819,\n",
       "         1012,  2033,  1007,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  4125,  1024, 23306,  1996,  2458,  1997,  2330,  3120,\n",
       "         4007,  2005, 15544, 11020,  1011,  1058,  1006, 11603, 14876,\n",
       "        18426,  3508,  1012,  7327,  1007,   102,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  2043,  5787,  1023,  2573,  2021,  5787,  2184, 17991,\n",
       "         1024,  1037,  2460,  2139,  8569, 12588,  2466,  1006, 16231,\n",
       "         1012,  6627,  1007,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  2637,  2038,  2205,  2172, 15960,  1006,  1059,  2015,\n",
       "         3501,  1012,  4012,  1007,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  2017,  2064,  4957,  2019,  2330, 26952,  2361,  3145,\n",
       "         2000,  1037,  2446,  1041,  3593,  1006, 21208,  5480,  2271,\n",
       "         1012,  2139,  1007,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101, 27838,  2389,  2003,  2019,  2125,  4179, 12653, 16602,\n",
       "         2005,  4007,  9797,  1006, 27838, 27318,  6169,  1012,  8917,\n",
       "         1007,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  9932, 17465, 13625, 14730,  2922, 28639,  3231,  7551,\n",
       "         2000,  3058,  1006,  9932, 17465,  1012,  4012,  1007,   102,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101, 16428,  2063,  1024,  1037,  9563,  7705,  2005, 27004,\n",
       "        26458,  2595,  1006,  7858,  1012,  4012,  1007,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  9932,  4950,  2007,  2053, 10014,  1006,  1996, 21572,\n",
       "        27718,  1012, 22834,  1007,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  2417, 23194, 17928, 20874,  2052,  3465,  9348,  9722,\n",
       "         1002,  2322,  2213,  2566,  2095,  1006, 15236, 10273,  7384,\n",
       "         1012,  5658,  1007,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  2265,  1044,  2078,  1024, 12702, 11834,  1516,  2797,\n",
       "         2177, 11834,  1006, 12702,  1012, 14163,  1007,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  2634,  7659, 15861,  2795,  1998,  6622,  2013,  2082,\n",
       "        18841,  1006,  3267,  1012,  4012,  1007,   102,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  9686, 14184,  1024,  1037,  9476,  2784,  1011, 11529,\n",
       "         1006,  2760,  1007,  1006, 20578,  2015,  1012,  9587,  5831,\n",
       "         4571,  1012,  8917,  1007,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  2425,  1044,  2078,  1024,  7858,  8816, 11744, 14489,\n",
       "         2013,  8224,  3945,  3463,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101, 23707,  2026,  1523,  6047,  1524, 11868, 18623,  1006,\n",
       "        13970,  2368,  5831,  1012, 16475,  1007,   102,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  2070,  4275,  1997, 15453, 21275,  2618,  2388, 15271,\n",
       "         8816,  3813,  8059, 14409, 16021, 29150,  2135,  1006, 17502,\n",
       "         1012,  4012,  1007,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  7596,  4755,  2167,  4759,  8645,  5871, 19119,  2046,\n",
       "         2712,  1006, 21025,  2480,  5302,  3527,  1012,  4012,  1007,\n",
       "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  9932, 20014,  6132, 14144,  2954,  2114,  1520,  3259,\n",
       "         6341,  1521,  2008, 14684,  6826,  2041,  8275,  2470,  1006,\n",
       "         3267,  1012,  4012,  1007,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101, 20704,  2595, 22203,  2475, 23807,  2015,  2005, 26219,\n",
       "         2243,  1521,  1055, 27448,  1012,  4066,  4725,  1006, 21025,\n",
       "         2705, 12083,  1012,  4012,  1013,  2330,  3501,  2094,  2243,\n",
       "         1007,   102,     0,     0,     0,     0],\n",
       "       [  101,  9532,  4419,  1010,  1997,  1996, 14815,  1011, 14535,\n",
       "         2316,  1996,  3901,  1006,  2672,  1007,  1006,  2760,  1007,\n",
       "         1006,  6396,  7292,  2015,  1012,  4012,  1007,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [  101,  3198,  1044,  2078,  1024,  2003,  2009,  2074,  2033,\n",
       "         2030, 14246,  2102,  1011,  1018,  1005,  1055,  3737,  2038,\n",
       "         6022, 20111,  9906,  1029,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers as ppb\n",
    "\n",
    "# pretrained_weights_name = \"distilbert-base-uncased\"\n",
    "pretrained_weights_name = \"bert-base-uncased\"\n",
    "tokenizer = ppb.DistilBertTokenizer.from_pretrained(pretrained_weights_name)\n",
    "model = ppb.DistilBertModel.from_pretrained(pretrained_weights_name)\n",
    "\n",
    "tokenized = df[\"title\"].apply((lambda s: tokenizer.encode(s, add_special_tokens=True)))\n",
    "\n",
    "max_len = max([len(row) for row in tokenized])\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "print(padded.shape)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 768)\n",
      "(29, 770)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 4.47852731e-01,  1.61816251e+00,  4.20298845e-01, ...,\n",
       "          2.13210061e-01,  1.94000000e+02,  7.70000000e+01],\n",
       "        [ 2.78446943e-01,  1.97484410e+00,  2.18175188e-01, ...,\n",
       "          5.90770841e-01,  6.90000000e+01,  4.10000000e+01],\n",
       "        [-6.53122887e-02,  1.70917666e+00,  9.57809836e-02, ...,\n",
       "          1.54796004e-01,  6.30000000e+01,  2.50000000e+01],\n",
       "        ...,\n",
       "        [ 6.22555375e-01,  1.70079875e+00,  2.26779312e-01, ...,\n",
       "          3.26630741e-01,  1.32000000e+02,  1.25000000e+02],\n",
       "        [ 5.93542278e-01,  1.46398520e+00,  3.04184079e-01, ...,\n",
       "          2.16497585e-01,  2.20000000e+01,  4.00000000e+00],\n",
       "        [ 5.43479562e-01,  2.00709319e+00,  3.45604748e-01, ...,\n",
       "          4.21110868e-01,  7.84000000e+02,  6.33000000e+02]]),\n",
       " [0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = torch.tensor(padded)\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(encoded, attention_mask=attention_mask)\n",
    "\n",
    "# last_hidden_states[0].shape\n",
    "\n",
    "\n",
    "\n",
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "print(features.shape)\n",
    "labels = df[\"viewed\"].to_list()\n",
    "\n",
    "# add features\n",
    "new_features = [\"points\",\"comments\"]\n",
    "features = np.append(features, df[new_features].to_numpy(), axis=1)\n",
    "print(features.shape)\n",
    "\n",
    "features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166666666666666\n",
      "0.609482170416332\n"
     ]
    }
   ],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "labels = df[\"viewed\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.4, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "clf = LogisticRegression(random_state=42)\n",
    "lin_clf = LinearRegression()\n",
    "clf.fit(x_train, y_train)\n",
    "print(clf.score(x_test, y_test))\n",
    "lin_clf.fit(x_train, y_train)\n",
    "print(lin_clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier score: 0.700 (+/- 0.08)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = DummyClassifier()\n",
    "scores = cross_val_score(clf, x_train, y_train)\n",
    "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
