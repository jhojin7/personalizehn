{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial1: 그냥 버트 써서 classifier 통과시키기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>visited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jraft – the best blogging platform ever</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>California, Love It and Leave It</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SpaceX Starlink has some hiccups as expected, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SpaceX Crew-1 [video]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Treehouse Is De-Plaformed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>U.S. Feds Seized Nearly $1B in Bitcoin from Wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>Facebook was used as a proxy by web scraping bots</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>How is Visual Basic still ranked #6 programmin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>Confessions of a voter fraud: I was a master a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>The Canadian Mental Health Association has end...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1492 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  visited\n",
       "0               Jraft – the best blogging platform ever        1\n",
       "1                      California, Love It and Leave It        0\n",
       "2     SpaceX Starlink has some hiccups as expected, ...        1\n",
       "3                                 SpaceX Crew-1 [video]        1\n",
       "4                         The Treehouse Is De-Plaformed        0\n",
       "...                                                 ...      ...\n",
       "1487  U.S. Feds Seized Nearly $1B in Bitcoin from Wa...        0\n",
       "1488  Facebook was used as a proxy by web scraping bots        1\n",
       "1489  How is Visual Basic still ranked #6 programmin...        0\n",
       "1490  Confessions of a voter fraud: I was a master a...        0\n",
       "1491  The Canadian Mental Health Association has end...        1\n",
       "\n",
       "[1492 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "# with open(\"data/fromgithub2.json\",\"r\") as f:\n",
    "#     df = pd.read_json(f)\n",
    "#     # posts = json.loads(f.read())\n",
    "\n",
    "# df = df[[\"title\",\"time\",\"url\"]]\n",
    "# df[\"time\"] = df[\"time\"].astype(\"datetime64[s]\",)\n",
    "# df[\"visited\"] = 0\n",
    "# # df.to_csv(\"data/fromgithub2.csv\")\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "FILE = \"data/database1.csv\"\n",
    "df = pd.read_csv(FILE)\n",
    "df = df[[\"title\",\"visited\"]]\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# # features_df = df[[\"title\",\"time\",\"kids\"]]\n",
    "# # titles = df.loc[df[\"title\"].isna()==False,\"title\"]\n",
    "# df.dropna(subset=[\"title\"])\n",
    "# titles = df[\"title\"]\n",
    "\n",
    "# # features_df.loc[:,\"kids\"] = features_df[\"kids\"].apply(lambda x: len(x))\n",
    "# # features_df[\"time\"] = features_df[\"time\"].astype('datetime64[s]')\n",
    "# titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((999,), (493,), (999,), (493,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # fill dummy data\n",
    "# y = [1 if i%2==0 else 0 for i in range(len(features))]\n",
    "# y = np.array(y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"title\"], df[\"visited\"], test_size=0.33, random_state=RANDOM_STATE)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel \n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "# tokenized = df[\"title\"].apply(\n",
    "#     (lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "tokenized_train = X_train.apply(\n",
    "    (lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "tokenized_test = X_test.apply(\n",
    "    (lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "tmp = max([len(row) for row in tokenized_train])\n",
    "max_len = max([len(row) for row in tokenized_test])\n",
    "max_len = max(tmp, max_len)\n",
    "\n",
    "\n",
    "padded_train = np.array([i + [0]*(max_len-len(i)) for i in tokenized_train.values])\n",
    "padded_test = np.array([i + [0]*(max_len-len(i)) for i in tokenized_test.values])\n",
    "attention_mask_train = np.where(padded_train!=0 , 1, 0)\n",
    "attention_mask_test = np.where(padded_test!=0 , 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "output_train = model(\n",
    "    torch.tensor(padded_train),\n",
    "    attention_mask=torch.tensor(attention_mask_train))\n",
    "# output_test = model(torch.tensor(padded_test), attention_mask=attention_mask_test)\n",
    "\n",
    "X_train = output_train[0][:,0,:].detach().numpy()\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(493, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_test = model(\n",
    "    torch.tensor(padded_test),\n",
    "    attention_mask=torch.tensor(attention_mask_test))\n",
    "\n",
    "X_test = output_test[0][:,0,:].detach().numpy()\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUMMY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8275862068965517"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"DUMMY\")\n",
    "clf = DummyClassifier(random_state=RANDOM_STATE)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "# cross_val_score(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR, LOGISTIC, RIDGE\n",
      "\n",
      "acc:  -4.677226181714537\n",
      "crossval:  [-1.94293948e+00 -1.18443346e+06 -2.11428898e+08 -1.72837814e+01\n",
      " -2.93420149e+00]\n",
      "auroc:  0.5695357554786621\n",
      "\n",
      "acc:  0.8316430020283976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crossval:  [0.83838384 0.82828283 0.82828283 0.82653061 0.82653061]\n",
      "auroc:  0.5117647058823529\n",
      "\n",
      "acc:  0.7991886409736308\n",
      "crossval:  [0.76767677 0.77777778 0.77777778 0.78571429 0.74489796]\n",
      "auroc:  0.5247549019607843\n",
      "\n",
      "acc:  0.6267748478701826\n",
      "crossval:  [0.63636364 0.67676768 0.56565657 0.57142857 0.59183673]\n",
      "auroc:  0.592892156862745\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.metrics import roc_curve, roc_auc_score, RocCurveDisplay\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"LINEAR, LOGISTIC, RIDGE\")\n",
    "# clf = LinearRegression()\n",
    "# clf = RidgeClassifier()\n",
    "clfs = [\n",
    "    LinearRegression(),\n",
    "    # LogisticRegression(max_iter=2, n_jobs=-1,random_state=RANDOM_STATE),\n",
    "    # LogisticRegression(),\n",
    "    LogisticRegression(max_iter=5, solver=\"sag\", n_jobs=-1,random_state=RANDOM_STATE),\n",
    "    RidgeClassifier(),\n",
    "    GaussianNB(),\n",
    "]\n",
    "\n",
    "\n",
    "for clf in clfs:\n",
    "    print()\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"acc: \", clf.score(X_test, y_test))\n",
    "    print(\"crossval: \", cross_val_score(clf, X_test, y_test))\n",
    "    # y_score = clf.pre(X_test)[:,1]\n",
    "    y_score = clf.predict(X_test)\n",
    "    print(\"auroc: \", roc_auc_score(y_test,y_score))\n",
    "    # fpr, tpr = roc_curve(y_test, y_score)\n",
    "    # display(RocCurveDisplay(fpr, tpr, ))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial2: fine tuing with torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((999,), (493,), (999,), (493,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "FILE = \"data/database1.csv\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "df = pd.read_csv(FILE)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"title\"], df[\"visited\"], test_size=0.33, random_state=RANDOM_STATE)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel \n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_text(X_text):\n",
    "    tokenized = X_text.apply(\n",
    "        (lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "    \n",
    "    max_len = max([len(row) for row in tokenized])\n",
    "    print(max_len)\n",
    "    # return text\n",
    "\n",
    "\n",
    "preprocess_text(X_train)\n",
    "preprocess_text(X_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
