{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'points', 'comments', 'viewed'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>points</th>\n",
       "      <th>comments</th>\n",
       "      <th>viewed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36153237</th>\n",
       "      <td>Learn x86-64 assembly by writing a GUI from sc...</td>\n",
       "      <td>29</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36152014</th>\n",
       "      <td>Ask HN: Who is hiring? (June 2023)</td>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36149462</th>\n",
       "      <td>I think Zig is hard but worth it (ratfactor.com)</td>\n",
       "      <td>232</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36140128</th>\n",
       "      <td>The unsung hero of the Apple Watch is its hidd...</td>\n",
       "      <td>120</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36151225</th>\n",
       "      <td>Show HN: StonksGPT – A Natural Language search...</td>\n",
       "      <td>65</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36152510</th>\n",
       "      <td>Measuring the productivity impact of generativ...</td>\n",
       "      <td>60</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36148807</th>\n",
       "      <td>Show HN: I made CSS Pro, a re-imagined Devtool...</td>\n",
       "      <td>305</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36151140</th>\n",
       "      <td>I went down the rabbit hole of buying GitHub S...</td>\n",
       "      <td>194</td>\n",
       "      <td>115.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36152222</th>\n",
       "      <td>Show HN: Discipline.io – Make binding commitme...</td>\n",
       "      <td>18</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36151220</th>\n",
       "      <td>Operation Triangulation: iOS Security Issue (s...</td>\n",
       "      <td>37</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36151417</th>\n",
       "      <td>Two Men Got Jobs at Amazon Just to Steal Copie...</td>\n",
       "      <td>92</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36148475</th>\n",
       "      <td>Distcc: A fast, free distributed C/C++ compile...</td>\n",
       "      <td>169</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36151917</th>\n",
       "      <td>Why Scientists Need to Get High (nautil.us)</td>\n",
       "      <td>19</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36149004</th>\n",
       "      <td>Security.txt file now mandatory for Dutch gove...</td>\n",
       "      <td>298</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36150503</th>\n",
       "      <td>Explore Australian Fossils in 3D (flinders.edu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36150443</th>\n",
       "      <td>Autoimmune disease can attack the brain, cause...</td>\n",
       "      <td>120</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36150543</th>\n",
       "      <td>Privacy: One of the Most Fundamental Human Rig...</td>\n",
       "      <td>122</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36146905</th>\n",
       "      <td>Lost John Coltrane recording, from experimenta...</td>\n",
       "      <td>260</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36149620</th>\n",
       "      <td>Show HN: Word2vec Algorithm in ~100sloc with N...</td>\n",
       "      <td>44</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36151056</th>\n",
       "      <td>Show HN: TodoBot is an AI coach that helps you...</td>\n",
       "      <td>14</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36151441</th>\n",
       "      <td>Using Adapt and Beam for Effective Data Modeli...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36148495</th>\n",
       "      <td>Introduction to Open Source Laptop Project by ...</td>\n",
       "      <td>102</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36152779</th>\n",
       "      <td>Blizzard CEO denies culture of harassment and ...</td>\n",
       "      <td>38</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36151286</th>\n",
       "      <td>Zuckerberg unveils Meta’s newest VR headset da...</td>\n",
       "      <td>57</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36153496</th>\n",
       "      <td>The Meta Quest 3 is a $499 mixed reality heads...</td>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36151132</th>\n",
       "      <td>People living near former atomic weapon plant ...</td>\n",
       "      <td>61</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36149690</th>\n",
       "      <td>Maryland License Plates Now Inadvertently Adve...</td>\n",
       "      <td>171</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36135250</th>\n",
       "      <td>Sun Remarketing Catalog Summer 1988 (archive.org)</td>\n",
       "      <td>66</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36149375</th>\n",
       "      <td>EPYC 7002 CPUs may hang after 1042 days of upt...</td>\n",
       "      <td>130</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      title  points  comments   \n",
       "id                                                                              \n",
       "36153237  Learn x86-64 assembly by writing a GUI from sc...      29       5.0  \\\n",
       "36152014                 Ask HN: Who is hiring? (June 2023)      70      60.0   \n",
       "36149462   I think Zig is hard but worth it (ratfactor.com)     232     136.0   \n",
       "36140128  The unsung hero of the Apple Watch is its hidd...     120      73.0   \n",
       "36151225  Show HN: StonksGPT – A Natural Language search...      65      27.0   \n",
       "36152510  Measuring the productivity impact of generativ...      60      59.0   \n",
       "36148807  Show HN: I made CSS Pro, a re-imagined Devtool...     305     172.0   \n",
       "36151140  I went down the rabbit hole of buying GitHub S...     194     115.0   \n",
       "36152222  Show HN: Discipline.io – Make binding commitme...      18      14.0   \n",
       "36151220  Operation Triangulation: iOS Security Issue (s...      37      10.0   \n",
       "36151417  Two Men Got Jobs at Amazon Just to Steal Copie...      92      68.0   \n",
       "36148475  Distcc: A fast, free distributed C/C++ compile...     169      85.0   \n",
       "36151917        Why Scientists Need to Get High (nautil.us)      19       5.0   \n",
       "36149004  Security.txt file now mandatory for Dutch gove...     298     132.0   \n",
       "36150503  Explore Australian Fossils in 3D (flinders.edu...       0       0.0   \n",
       "36150443  Autoimmune disease can attack the brain, cause...     120      79.0   \n",
       "36150543  Privacy: One of the Most Fundamental Human Rig...     122      81.0   \n",
       "36146905  Lost John Coltrane recording, from experimenta...     260      43.0   \n",
       "36149620  Show HN: Word2vec Algorithm in ~100sloc with N...      44      15.0   \n",
       "36151056  Show HN: TodoBot is an AI coach that helps you...      14       9.0   \n",
       "36151441  Using Adapt and Beam for Effective Data Modeli...       0       0.0   \n",
       "36148495  Introduction to Open Source Laptop Project by ...     102      48.0   \n",
       "36152779  Blizzard CEO denies culture of harassment and ...      38      17.0   \n",
       "36151286  Zuckerberg unveils Meta’s newest VR headset da...      57      49.0   \n",
       "36153496  The Meta Quest 3 is a $499 mixed reality heads...       8       2.0   \n",
       "36151132  People living near former atomic weapon plant ...      61      17.0   \n",
       "36149690  Maryland License Plates Now Inadvertently Adve...     171      85.0   \n",
       "36135250  Sun Remarketing Catalog Summer 1988 (archive.org)      66      23.0   \n",
       "36149375  EPYC 7002 CPUs may hang after 1042 days of upt...     130      82.0   \n",
       "\n",
       "          viewed  \n",
       "id                \n",
       "36153237       1  \n",
       "36152014       0  \n",
       "36149462       1  \n",
       "36140128       0  \n",
       "36151225       1  \n",
       "36152510       1  \n",
       "36148807       0  \n",
       "36151140       1  \n",
       "36152222       0  \n",
       "36151220       0  \n",
       "36151417       1  \n",
       "36148475       0  \n",
       "36151917       1  \n",
       "36149004       0  \n",
       "36150503       0  \n",
       "36150443       0  \n",
       "36150543       0  \n",
       "36146905       0  \n",
       "36149620       1  \n",
       "36151056       1  \n",
       "36151441       0  \n",
       "36148495       0  \n",
       "36152779       0  \n",
       "36151286       0  \n",
       "36153496       0  \n",
       "36151132       0  \n",
       "36149690       0  \n",
       "36135250       0  \n",
       "36149375       0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"features.csv\", index_col=\"id\")\n",
    "print(df.columns)\n",
    "# df = df[[\"title\", \"points\", \"comments\", \"viewed\"]]\n",
    "# df[\"viewed\"] = df[\"viewed\"].apply((lambda x: 1 if x==1 else 0))\n",
    "# df.loc[df[\"viewed\"].isna(), \"viewed\"] = 0\n",
    "# df.loc[[1,3,10,12,18,19,21],\"viewed\"] = 1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n",
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing DistilBertModel: ['bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'cls.predictions.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['transformer.layer.2.attention.k_lin.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.9.output_layer_norm.weight', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.11.attention.v_lin.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.9.ffn.lin2.bias', 'embeddings.word_embeddings.weight', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.7.ffn.lin1.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.1.output_layer_norm.weight', 'embeddings.position_embeddings.weight', 'embeddings.LayerNorm.weight', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.0.attention.q_lin.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  101,  4553,  1060, 20842,  1011,  4185,  3320,  2011,  3015,\n",
       "         1037, 26458,  2013, 11969,  1006, 26522, 17579,  1012, 21025,\n",
       "         2705, 12083,  1012, 22834,  1007,   102,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  3198,  1044,  2078,  1024,  2040,  2003, 14763,  1029,\n",
       "         1006,  2238, 16798,  2509,  1007,   102,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  1045,  2228,  1062,  8004,  2003,  2524,  2021,  4276,\n",
       "         2009,  1006,  9350,  7011, 16761,  1012,  4012,  1007,   102,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  1996,  4895, 19729,  2290,  5394,  1997,  1996,  6207,\n",
       "         3422,  2003,  2049,  5023, 11287,  1006,  1996,  6299,  3351,\n",
       "         1012,  4012,  1007,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  2265,  1044,  2078,  1024,  2358,  2239,  5705, 21600,\n",
       "         2102,  1516,  1037,  3019,  2653,  3945,  6994,  2005, 15768,\n",
       "         1998,  5446,  2951,  1006,  2358,  2239,  5705,  1012,  2739,\n",
       "         1007,   102,     0,     0,     0],\n",
       "       [  101,  9854,  1996, 15836,  4254,  1997, 11416,  6024,  9932,\n",
       "         1006,  1050,  5677,  1012,  8917,  1007,   102,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  2265,  1044,  2078,  1024,  1045,  2081, 20116,  2015,\n",
       "         4013,  1010,  1037,  2128,  1011,  8078, 16475,  3406, 27896,\n",
       "         2005,  4773,  2640,  1006, 20116, 13102,  3217,  1012,  4012,\n",
       "         1007,   102,     0,     0,     0],\n",
       "       [  101,  1045,  2253,  2091,  1996, 10442,  4920,  1997,  9343,\n",
       "        21025,  2705, 12083,  3340,  1010,  2061,  2017,  2180,  1005,\n",
       "         1056,  2031,  2000,  1006,  1996,  1011,  9054,  1012, 16475,\n",
       "         1007,   102,     0,     0,     0],\n",
       "       [  101,  2265,  1044,  2078,  1024,  9009,  1012, 22834,  1516,\n",
       "         2191,  8031, 17786,  2000,  2115,  2488,  2969,  1006,  9009,\n",
       "         1012, 22834,  1007,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  3169, 13012,  5654,  9513,  1024, 16380,  3036,  3277,\n",
       "         1006,  5851,  9863,  1012,  4012,  1007,   102,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  2048,  2273,  2288,  5841,  2012,  9733,  2074,  2000,\n",
       "         8954,  4809,  1997, 27838, 15150,  1006, 23856,  5283,  1012,\n",
       "         4012,  1007,   102,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  4487,  3367,  9468,  1024,  1037,  3435,  1010,  2489,\n",
       "         5500,  1039,  1013,  1039,  1009,  1009, 21624,  1006,  4487,\n",
       "         3367,  9468,  1012,  8917,  1007,   102,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  2339,  6529,  2342,  2000,  2131,  2152,  1006,  6583,\n",
       "        21823,  2140,  1012,  2149,  1007,   102,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  3036,  1012, 19067,  2102,  5371,  2085, 10915,  2005,\n",
       "         3803,  2231, 11744,  1006,  8466,  2368,  1012,  4012,  1007,\n",
       "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  8849,  2827, 11954,  1999,  7605,  1006, 29120,  1012,\n",
       "         3968,  2226,  1012,  8740,  1007,   102,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  8285,  5714, 23041,  2063,  4295,  2064,  2886,  1996,\n",
       "         4167,  1010,  3426, 13691,  8030,  1006,  2899, 19894,  1012,\n",
       "         4012,  1007,   102,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  9394,  1024,  2028,  1997,  1996,  2087,  8050,  2529,\n",
       "         2916,  1010,  2664,  7887,  2104,  5081,  1006,  4019,  5638,\n",
       "        13512, 15937,  1012, 18558,  1007,   102,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  2439,  2198,  9110, 18053,  3405,  1010,  2013,  6388,\n",
       "         4403,  2007,  4388,  2079, 14277, 10536,  1010, 19391,  1006,\n",
       "        21411,  1012,  8917,  1007,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  2265,  1044,  2078,  1024,  2773,  2475,  3726,  2278,\n",
       "         9896,  1999,  1066,  2531, 14540, 10085,  2007, 16371,  8737,\n",
       "         2100,  1006, 21025,  2705, 12083,  1012,  4012,  1013,  3312,\n",
       "        19022,  6977,  2571,  1007,   102],\n",
       "       [  101,  2265,  1044,  2078,  1024, 28681, 16429,  4140,  2003,\n",
       "         2019,  9932,  2873,  2008,  7126,  2017,  4339,  1037,  2488,\n",
       "        28681,  2080,  2862,  1006, 28681, 16429,  4140,  1012,  9932,\n",
       "         1007,   102,     0,     0,     0],\n",
       "       [  101,  2478, 15581,  1998,  7504,  2005,  4621,  2951, 11643,\n",
       "         1006,  2250,  3762,  2618,  1012,  4012,  1007,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  4955,  2000,  2330,  3120, 12191,  2622,  2011, 23739,\n",
       "        21863, 11705,  1006, 12456,  5007,  1012,  4012,  1007,   102,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101, 21689,  5766, 23439,  3226,  1997, 16011,  1998,  7499,\n",
       "         2015,  9209,  2005,  2194,  3471,  1006,  1996,  6299,  3351,\n",
       "         1012,  4012,  1007,   102,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101, 16950,  9102,  4059,  4895,  3726, 12146, 18804,  1521,\n",
       "         1055, 14751, 27830,  4641,  3388,  2420,  2077,  6207,  7657,\n",
       "         2049,  2219,  1006, 27166,  9818,  1012,  4012,  1007,   102,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  1996, 18804,  8795,  1017,  2003,  1037,  1002,  4749,\n",
       "         2683,  3816,  4507,  4641,  3388,  2007,  2440,  1011,  3609,\n",
       "         3413,  2705, 22494,  5603,  1006, 25540,  4215, 18150,  1012,\n",
       "         4012,  1007,   102,     0,     0],\n",
       "       [  101,  2111,  2542,  2379,  2280,  9593,  5195,  3269,  1999,\n",
       "        11333,  2031,  3445,  4456,  6165,  1006,  9152,  2232,  1012,\n",
       "        18079,  1007,   102,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  5374,  6105,  7766,  2085, 21089,  6475, 10275,  3784,\n",
       "         9270,  1006,  3580,  1012,  4012,  1007,   102,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  3103, 17674, 20624,  3070, 12105,  2621,  2997,  1006,\n",
       "         8756,  1012,  8917,  1007,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0],\n",
       "       [  101,  4958,  2100,  2278,  6352,  2475, 17368,  2015,  2089,\n",
       "         6865,  2044,  9645,  2475,  2420,  1997,  2039,  7292,  1006,\n",
       "         2417, 23194,  1012,  4012,  1007,   102,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers as ppb\n",
    "\n",
    "# pretrained_weights_name = \"distilbert-base-uncased\"\n",
    "pretrained_weights_name = \"bert-base-uncased\"\n",
    "tokenizer = ppb.DistilBertTokenizer.from_pretrained(pretrained_weights_name)\n",
    "model = ppb.DistilBertModel.from_pretrained(pretrained_weights_name)\n",
    "\n",
    "tokenized = df[\"title\"].apply((lambda s: tokenizer.encode(s, add_special_tokens=True)))\n",
    "\n",
    "max_len = max([len(row) for row in tokenized])\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "print(padded.shape)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 768)\n",
      "(29, 770)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 4.77928609e-01,  4.69486266e-02,  2.63111353e-01, ...,\n",
       "          8.85494173e-01,  2.90000000e+01,  5.00000000e+00],\n",
       "        [ 7.20696926e-01,  7.27723897e-01, -1.50537387e-01, ...,\n",
       "          7.92951703e-01,  7.00000000e+01,  6.00000000e+01],\n",
       "        [ 4.33123648e-01,  3.28972697e-01, -3.31434421e-04, ...,\n",
       "          1.32451868e+00,  2.32000000e+02,  1.36000000e+02],\n",
       "        ...,\n",
       "        [ 4.92828727e-01,  3.69941354e-01,  6.20662645e-02, ...,\n",
       "          8.58972430e-01,  1.71000000e+02,  8.50000000e+01],\n",
       "        [ 4.70090955e-01,  5.47375560e-01, -5.69418848e-01, ...,\n",
       "          1.16505075e+00,  6.60000000e+01,  2.30000000e+01],\n",
       "        [ 5.20785928e-01,  3.47302794e-01,  7.54495859e-02, ...,\n",
       "          7.99164414e-01,  1.30000000e+02,  8.20000000e+01]]),\n",
       " [1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = torch.tensor(padded)\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(encoded, attention_mask=attention_mask)\n",
    "\n",
    "# last_hidden_states[0].shape\n",
    "\n",
    "\n",
    "\n",
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "print(features.shape)\n",
    "labels = df[\"viewed\"].to_list()\n",
    "\n",
    "# add features\n",
    "new_features = [\"points\",\"comments\"]\n",
    "features = np.append(features, df[new_features].to_numpy(), axis=1)\n",
    "print(features.shape)\n",
    "\n",
    "features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "-0.9242878449635046\n"
     ]
    }
   ],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "labels = df[\"viewed\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.4, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "clf = LogisticRegression(random_state=42)\n",
    "lin_clf = LinearRegression()\n",
    "clf.fit(x_train, y_train)\n",
    "print(clf.score(x_test, y_test))\n",
    "lin_clf.fit(x_train, y_train)\n",
    "print(lin_clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier score: 0.600 (+/- 0.16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = DummyClassifier()\n",
    "scores = cross_val_score(clf, x_train, y_train)\n",
    "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
