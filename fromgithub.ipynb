{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>visited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jraft – the best blogging platform ever</td>\n",
       "      <td>2020-11-16 02:12:45</td>\n",
       "      <td>http://www.tryjraft.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>California, Love It and Leave It</td>\n",
       "      <td>2020-11-16 01:33:38</td>\n",
       "      <td>https://www.wsj.com/articles/california-love-i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SpaceX Starlink has some hiccups as expected, ...</td>\n",
       "      <td>2020-11-16 00:56:03</td>\n",
       "      <td>https://arstechnica.com/information-technology...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SpaceX Crew-1 [video]</td>\n",
       "      <td>2020-11-16 00:23:38</td>\n",
       "      <td>https://www.youtube.com/watch?v=bnChQbxLkkI&amp;re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Treehouse Is De-Plaformed</td>\n",
       "      <td>2020-11-15 23:28:32</td>\n",
       "      <td>https://theconservativetreehouse.com/2020/11/1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>U.S. Feds Seized Nearly $1B in Bitcoin from Wa...</td>\n",
       "      <td>2020-11-05 18:22:07</td>\n",
       "      <td>https://www.vice.com/en/article/akdgz8/us-feds...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>Facebook was used as a proxy by web scraping bots</td>\n",
       "      <td>2020-11-05 18:15:42</td>\n",
       "      <td>https://datadome.co/bot-detection/how-facebook...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>How is Visual Basic still ranked #6 programmin...</td>\n",
       "      <td>2020-11-05 18:06:53</td>\n",
       "      <td>https://www.tiobe.com/tiobe-index/</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>Confessions of a voter fraud: I was a master a...</td>\n",
       "      <td>2020-11-05 17:48:36</td>\n",
       "      <td>https://nypost.com/2020/08/29/political-inside...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>The Canadian Mental Health Association has end...</td>\n",
       "      <td>2020-11-05 17:50:11</td>\n",
       "      <td>https://cmha.ca/news/the-universal-basic-incom...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1492 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title                time   \n",
       "0               Jraft – the best blogging platform ever 2020-11-16 02:12:45  \\\n",
       "1                      California, Love It and Leave It 2020-11-16 01:33:38   \n",
       "2     SpaceX Starlink has some hiccups as expected, ... 2020-11-16 00:56:03   \n",
       "3                                 SpaceX Crew-1 [video] 2020-11-16 00:23:38   \n",
       "4                         The Treehouse Is De-Plaformed 2020-11-15 23:28:32   \n",
       "...                                                 ...                 ...   \n",
       "1487  U.S. Feds Seized Nearly $1B in Bitcoin from Wa... 2020-11-05 18:22:07   \n",
       "1488  Facebook was used as a proxy by web scraping bots 2020-11-05 18:15:42   \n",
       "1489  How is Visual Basic still ranked #6 programmin... 2020-11-05 18:06:53   \n",
       "1490  Confessions of a voter fraud: I was a master a... 2020-11-05 17:48:36   \n",
       "1491  The Canadian Mental Health Association has end... 2020-11-05 17:50:11   \n",
       "\n",
       "                                                    url  visited  \n",
       "0                               http://www.tryjraft.com        0  \n",
       "1     https://www.wsj.com/articles/california-love-i...        0  \n",
       "2     https://arstechnica.com/information-technology...        0  \n",
       "3     https://www.youtube.com/watch?v=bnChQbxLkkI&re...        0  \n",
       "4     https://theconservativetreehouse.com/2020/11/1...        0  \n",
       "...                                                 ...      ...  \n",
       "1487  https://www.vice.com/en/article/akdgz8/us-feds...        0  \n",
       "1488  https://datadome.co/bot-detection/how-facebook...        0  \n",
       "1489                 https://www.tiobe.com/tiobe-index/        0  \n",
       "1490  https://nypost.com/2020/08/29/political-inside...        0  \n",
       "1491  https://cmha.ca/news/the-universal-basic-incom...        0  \n",
       "\n",
       "[1492 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open(\"data/fromgithub2.json\",\"r\") as f:\n",
    "    df = pd.read_json(f)\n",
    "    # posts = json.loads(f.read())\n",
    "\n",
    "df = df[[\"title\",\"time\",\"url\"]]\n",
    "df[\"time\"] = df[\"time\"].astype(\"datetime64[s]\",)\n",
    "df[\"visited\"] = 0\n",
    "# df.to_csv(\"data/fromgithub2.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    We Don’t Need Elections to Figure Out What Peo...\n",
       "1                Hegel and the History of Human Nature\n",
       "2                       Ask HN: What's Life All About?\n",
       "3        Ask HN: What do Stoics think of Ecclesiastes?\n",
       "4                        The Problem of Centralisation\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# features_df = df[[\"title\",\"time\",\"kids\"]]\n",
    "# titles = df.loc[df[\"title\"].isna()==False,\"title\"]\n",
    "df.dropna(subset=[\"title\"])\n",
    "titles = df[\"title\"]\n",
    "\n",
    "# features_df.loc[:,\"kids\"] = features_df[\"kids\"].apply(lambda x: len(x))\n",
    "# features_df[\"time\"] = features_df[\"time\"].astype('datetime64[s]')\n",
    "titles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  101,  2057,  2123, ...,     0,     0,     0],\n",
       "       [  101,  2002, 12439, ...,     0,     0,     0],\n",
       "       [  101,  3198,  1044, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  2017,  2453, ...,     0,     0,     0],\n",
       "       [  101, 19657,  5448, ...,     0,     0,     0],\n",
       "       [  101, 18546,  9832, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel \n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "tokenized = titles.apply(\n",
    "    (lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "max_len = max([len(row) for row in tokenized])\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "attention_mask = np.where(padded!=0 , 1, 0)\n",
    "\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " torch.Size([600, 31]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "padded = torch.tensor(padded)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "attention_mask, attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[ 0.0771,  0.0182, -0.0643,  ..., -0.1913,  0.1890,  0.3527],\n",
       "         [ 0.4723, -0.0055, -0.2808,  ..., -0.2538,  0.3928, -0.1306],\n",
       "         [-0.0460,  0.2772,  0.3011,  ..., -0.5342,  0.0617, -0.2683],\n",
       "         ...,\n",
       "         [ 0.2057, -0.2744, -0.1650,  ..., -0.0303, -0.1430,  0.0469],\n",
       "         [ 0.3543, -0.0234,  0.0338,  ..., -0.1402,  0.0170, -0.3511],\n",
       "         [ 0.1917,  0.1626, -0.0960,  ..., -0.2867, -0.0862, -0.2281]],\n",
       "\n",
       "        [[-0.3856,  0.0923, -0.5885,  ..., -0.1052,  0.3748,  0.5154],\n",
       "         [ 0.0447,  0.0093, -0.0174,  ...,  0.0485,  0.4887, -0.2297],\n",
       "         [-0.0884,  0.6354, -0.4877,  ..., -0.2014, -0.1107,  0.6105],\n",
       "         ...,\n",
       "         [-0.0048,  0.2007, -0.2539,  ...,  0.2085,  0.0093,  0.3343],\n",
       "         [-0.2145,  0.2177, -0.1630,  ..., -0.0419,  0.2994,  0.3233],\n",
       "         [-0.2263,  0.3092, -0.1494,  ..., -0.1419,  0.2361,  0.1098]],\n",
       "\n",
       "        [[ 0.0413, -0.0265, -0.1379,  ...,  0.0640,  0.3722,  0.3841],\n",
       "         [ 0.0575,  0.0551, -0.0093,  ...,  0.0129,  0.3679, -0.0057],\n",
       "         [ 0.6259, -0.1629,  0.5254,  ...,  0.1022,  0.4672,  0.0181],\n",
       "         ...,\n",
       "         [ 0.1328,  0.0407,  0.1758,  ...,  0.1162,  0.1298,  0.0122],\n",
       "         [ 0.0575,  0.0792,  0.1495,  ...,  0.0510, -0.1479, -0.0098],\n",
       "         [ 0.2542,  0.1839,  0.2062,  ...,  0.1334,  0.1128,  0.0670]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.2041, -0.2029, -0.1836,  ..., -0.1842,  0.1202,  0.4585],\n",
       "         [-0.5715, -0.2808,  0.1002,  ..., -0.0608,  0.2227,  0.0969],\n",
       "         [-0.0683, -0.2638,  0.2661,  ..., -0.4705, -0.0828,  0.0683],\n",
       "         ...,\n",
       "         [ 0.2164, -0.0657,  0.0432,  ..., -0.1312, -0.0702, -0.0171],\n",
       "         [ 0.3907,  0.2343, -0.0511,  ..., -0.1298, -0.2794, -0.0640],\n",
       "         [ 0.1722,  0.1394, -0.0627,  ..., -0.2303, -0.0883,  0.0064]],\n",
       "\n",
       "        [[-0.0907, -0.2601, -0.0165,  ..., -0.0541,  0.5283,  0.4104],\n",
       "         [-0.0495, -0.0281,  0.2169,  ...,  0.1541,  0.2159,  0.2366],\n",
       "         [ 0.0162, -0.2428,  0.0645,  ...,  0.0969,  0.1390,  0.0684],\n",
       "         ...,\n",
       "         [ 0.1424, -0.2989,  0.1955,  ...,  0.2416,  0.0567,  0.0222],\n",
       "         [ 0.0498, -0.2238,  0.5343,  ..., -0.1546, -0.0077, -0.1107],\n",
       "         [-0.0377, -0.3102,  0.4320,  ..., -0.0144,  0.0076,  0.2429]],\n",
       "\n",
       "        [[-0.4050, -0.3327,  0.1736,  ..., -0.1366,  0.2309,  0.3017],\n",
       "         [ 0.3415,  0.1080,  0.2172,  ...,  0.0233,  0.2933,  0.1400],\n",
       "         [ 0.3151, -0.2272,  0.3309,  ...,  0.0499, -0.0996, -0.2595],\n",
       "         ...,\n",
       "         [ 0.0404, -0.1084,  0.2998,  ...,  0.0228, -0.1340,  0.0891],\n",
       "         [-0.1954, -0.0145,  0.4256,  ..., -0.0996,  0.1031,  0.0781],\n",
       "         [-0.1254,  0.0515,  0.4691,  ...,  0.0205,  0.0739, -0.1870]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(padded, attention_mask=attention_mask)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.07705708,  0.01822153, -0.06425738, ..., -0.19133918,\n",
       "         0.18901303,  0.35266334],\n",
       "       [-0.38561448,  0.09233379, -0.58854973, ..., -0.10521927,\n",
       "         0.37476808,  0.51544446],\n",
       "       [ 0.0412614 , -0.02650118, -0.13785702, ...,  0.06401408,\n",
       "         0.37219882,  0.384129  ],\n",
       "       ...,\n",
       "       [-0.20407574, -0.20291403, -0.18362422, ..., -0.1841933 ,\n",
       "         0.12020883,  0.4585222 ],\n",
       "       [-0.09072461, -0.26008007, -0.01645303, ..., -0.05409419,\n",
       "         0.5282614 ,  0.41041008],\n",
       "       [-0.4049973 , -0.33266595,  0.17361143, ..., -0.13655995,\n",
       "         0.23092951,  0.30166227]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = output[0][:,0,:].detach().numpy()\n",
    "print(features.shape)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill dummy data\n",
    "y = [1 if i%2==0 else 0 for i in range(len(features))]\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((402, 768), (198, 768), (402,), (198,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, y, test_size=0.33, random_state=RANDOM_STATE)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUMMY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.45454545454545453"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"DUMMY\")\n",
    "clf = DummyClassifier(random_state=RANDOM_STATE)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "# cross_val_score(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR, LOGISTIC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-8.588361918695732"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"LINEAR, LOGISTIC\")\n",
    "clf = LinearRegression()\n",
    "# clf = LogisticRegression(random_state=RANDOM_STATE)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "# cross_val_score(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAIVE BAYES\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5202020202020202"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"NAIVE BAYES\")\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "# cross_val_score(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n##############################33333\\n# column tranformer for preprocessing. TRASH.ed.\\n##############################33333\\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\\nmodel = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\\n\\nfrom sklearn.compose import ColumnTransformer\\n\\n# # titles = df[\"title\"]\\n# # titles\\n# class TextTransformer:\\n#     def __init__(self, tokenizer, model):\\n#         self.tokenizer = tokenizer\\n#         self.model = model\\n    \\n#     def fit(self, X, y=None):\\n#         return self\\n\\n#     def transform(self, X):\\n#         tokenized = titles.apply(\\n#             (lambda x: tokenizer.encode(x, add_special_tokens=True)))\\n#         max_len = max([len(row) for row in tokenized])\\n#         padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\\n#         attention_mask = np.where(padded!=0 , 1, 0)\\n#         padded = torch.tensor(padded)\\n#         attention_mask = torch.tensor(attention_mask)\\n#         output = model(padded, attention_mask=attention_mask)\\n#         features = output[0][:,0,:].detach().numpy()\\n#         return features\\n\\n\\n# preprocessor = ColumnTransformer([\\n#     (\"text_trasnform\", TextTransformer, [\"title\"])\\n#     ],\\n#     verbose=True,\\n# )\\n\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.linear_model import LogisticRegression\\n# pipeline = Pipeline([\\n#     (\"column_transformer\",preprocessor),\\n#     (\"classifier\", LogisticRegression()),\\n# ])\\n\\n# # 36153496,The Meta Quest 3 is a $499 mixed reality headset with full-color passthrough (engadget.com),8,2.0,0\\n# # 36151132,People living near former atomic weapon plant in WA have increased cancer rates (nih.gov),61,17.0,0\\n# # 36149690,Maryland License Plates Now Inadvertently Advertising Filipino Online Casino (vice.com),171,85.0,0\\n# # 36135250,Sun Remarketing Catalog Summer 1988 (archive.org),66,23.0,0\\n# # 36149375,EPYC 7002 CPUs may hang after 1042 days of uptime (reddit.com),130,82.0,0\\n\\n\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\n# Create a pipeline\\ntokenizer = DistilBertTokenizer.from_pretrained(\\'distilbert-base-uncased\\')\\nmodel = DistilBertModel.from_pretrained(\\'distilbert-base-uncased\\')\\n\\ntext_transformer = Pipeline([\\n    (\\'tokenizer\\', tokenizer),\\n    (\\'vectorizer\\', TfidfVectorizer(tokenizer=tokenizer.tokenize)),\\n])\\n\\npreprocessor = ColumnTransformer([\\n    (\\'text\\', text_transformer, \\'title\\')\\n])\\n\\npipeline = Pipeline([\\n    (\\'preprocessor\\', preprocessor),\\n    (\\'classifier\\', LogisticRegression())\\n])\\n\\n\\ny = [1 if i%2==0 else 0 for i in range(len(features))]\\ny = np.array(y)\\n\\nfrom sklearn.model_selection import train_test_split\\nRANDOM_STATE = 42\\nX_train, X_test, y_train, y_test = train_test_split(\\n    df[[\"title\"]], y, test_size=0.2, random_state=RANDOM_STATE\\n)\\n\\nprint(X_train.shape,X_test.shape, y_train.shape, y_test.shape)\\nprint(y_train)\\n\\npipeline.fit(X_train, y_train)\\n# pipeline.predict(X_test)\\n\\n# pipeline.transform([\"A very clickbaity title from Tesla, Apple, Google.\"])\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "##############################33333\n",
    "# column tranformer for preprocessing. TRASH.ed.\n",
    "##############################33333\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# # titles = df[\"title\"]\n",
    "# # titles\n",
    "# class TextTransformer:\n",
    "#     def __init__(self, tokenizer, model):\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.model = model\n",
    "    \n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         tokenized = titles.apply(\n",
    "#             (lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "#         max_len = max([len(row) for row in tokenized])\n",
    "#         padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "#         attention_mask = np.where(padded!=0 , 1, 0)\n",
    "#         padded = torch.tensor(padded)\n",
    "#         attention_mask = torch.tensor(attention_mask)\n",
    "#         output = model(padded, attention_mask=attention_mask)\n",
    "#         features = output[0][:,0,:].detach().numpy()\n",
    "#         return features\n",
    "\n",
    "\n",
    "# preprocessor = ColumnTransformer([\n",
    "#     (\"text_trasnform\", TextTransformer, [\"title\"])\n",
    "#     ],\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# pipeline = Pipeline([\n",
    "#     (\"column_transformer\",preprocessor),\n",
    "#     (\"classifier\", LogisticRegression()),\n",
    "# ])\n",
    "\n",
    "# # 36153496,The Meta Quest 3 is a $499 mixed reality headset with full-color passthrough (engadget.com),8,2.0,0\n",
    "# # 36151132,People living near former atomic weapon plant in WA have increased cancer rates (nih.gov),61,17.0,0\n",
    "# # 36149690,Maryland License Plates Now Inadvertently Advertising Filipino Online Casino (vice.com),171,85.0,0\n",
    "# # 36135250,Sun Remarketing Catalog Summer 1988 (archive.org),66,23.0,0\n",
    "# # 36149375,EPYC 7002 CPUs may hang after 1042 days of uptime (reddit.com),130,82.0,0\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a pipeline\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "text_transformer = Pipeline([\n",
    "    ('tokenizer', tokenizer),\n",
    "    ('vectorizer', TfidfVectorizer(tokenizer=tokenizer.tokenize)),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('text', text_transformer, 'title')\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "\n",
    "y = [1 if i%2==0 else 0 for i in range(len(features))]\n",
    "y = np.array(y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "RANDOM_STATE = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[[\"title\"]], y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(X_train.shape,X_test.shape, y_train.shape, y_test.shape)\n",
    "print(y_train)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "# pipeline.predict(X_test)\n",
    "\n",
    "# pipeline.transform([\"A very clickbaity title from Tesla, Apple, Google.\"])\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
